# -*- coding: utf-8 -*-
"""Ticket priority.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nG25IkSWdRv2K9W3ii7aUTEj583yxwzC
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import pandas as pd

# Correct file path
file_path = '/content/drive/MyDrive/priority tickets.csv'
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
df.head()

# Ensure 'Entered On' is in datetime format
df['Entered On'] = pd.to_datetime(df['Entered On'])

# Replace NaN values in 'Required On' with 'Entered On' + 10 days
df['Required On'].fillna(df['Entered On'] + pd.Timedelta(days=10), inplace=True)

# Display the updated DataFrame to verify the changes
df.head()

"""Going with random forest"""

import pandas as pd
from datetime import timedelta



# Convert the 'Required On' column to datetime format
df['Required On'] = pd.to_datetime(df['Required On'], errors='coerce')

# Define the entered date (example: '2024-11-01')
entered_date = pd.to_datetime('2024-11-01')

# Replace NaN values in 'Required On' with entered date + 10 days
df['Required On'] = df['Required On'].fillna(entered_date + timedelta(days=10))

print(df)

# Filter DataFrame by 'User ID'
user_ids = ['jjuscamaita', 'jzuniga', 'lingram', 'cminaya', 'dwest']
filtered_df = df[df['Note Created By UserID'].isin(user_ids)]

# Display the filtered DataFrame
print(filtered_df)

# Replace all NaN values in the filtered DataFrame with "Nothing"
filtered_df = filtered_df.fillna("Nothing")

# Display the filtered DataFrame
print(filtered_df)

# Assuming the column 'Ticket Type' is in the DataFrame 'filtered_df'
ticket_types = filtered_df['Ticket Type'].unique()

# Display the unique ticket types
print(ticket_types)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Assuming 'filtered_df' is your original DataFrame
# Load your DataFrame here
# filtered_df = pd.read_csv('your_data.csv')  # Example of loading data

# Create a subset DataFrame with the specified columns
subset_columns = ["Severity", "Required On", "Entered On", "Ticket Type", "Ticket Labor Hours","Note Created By UserID"]
subset_df = filtered_df[subset_columns]

# Display the first few rows of the subset DataFrame (optional)
print("Subset DataFrame:")
print(subset_df.head())

"""Moving forward with logistic regression"""

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt

# Assuming subset_df is already defined and contains the necessary columns
# Display the first few rows of the subset DataFrame
print("Subset DataFrame:")
print(subset_df.head())

# Convert date columns to datetime format
subset_df['Required On'] = pd.to_datetime(subset_df['Required On'])
subset_df['Entered On'] = pd.to_datetime(subset_df['Entered On'])

# Group by 'Required On' and count tickets
ticket_counts = subset_df.groupby('Required On').size()

# Convert to DataFrame for easier manipulation if needed
ticket_counts_df = ticket_counts.reset_index(name='Ticket Count')

# Set 'Required On' as index for time series analysis
ticket_counts_df.set_index('Required On', inplace=True)

# Fit an ARIMA model (example parameters)
model = ARIMA(ticket_counts_df['Ticket Count'], order=(1, 1, 1))  # Adjust parameters as needed
model_fit = model.fit()

# Forecast future ticket counts (e.g., next 30 days)
forecast_steps = 30
forecast = model_fit.forecast(steps=forecast_steps)

# Print forecast results
print("\nForecasted Ticket Counts for the Next 30 Days:")
print(forecast)

# Plotting the results
plt.figure(figsize=(10, 5))
plt.plot(ticket_counts_df.index, ticket_counts_df['Ticket Count'], label='Historical Ticket Counts')
plt.plot(pd.date_range(start=ticket_counts_df.index[-1] + pd.Timedelta(days=1), periods=forecast_steps), forecast, label='Forecasted Ticket Counts', color='orange')
plt.title('Ticket Counts Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Tickets')
plt.legend()
plt.grid()
plt.show()

# Prioritize tickets based on severity and due dates
prioritized_tickets = subset_df.sort_values(by=['Severity', 'Required On'])
print("\nPrioritized Tickets:")
print(prioritized_tickets)

# Assuming prioritized_tickets is already defined
# Prioritize tickets based on severity and due dates
prioritized_tickets = subset_df.sort_values(by=['Severity', 'Required On'])

# Save the prioritized tickets to a CSV file
prioritized_tickets.to_csv('prioritized_tickets.csv', index=False)

print("Prioritized tickets have been saved to 'prioritized_tickets.csv'.")

# Assuming subset_df is already defined

# Display unique values in the 'Severity' column
unique_severity_values = subset_df['Severity'].unique()

print("Unique Severity Values:")
print(unique_severity_values)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Provided list of severity levels
severity_levels = [
    'Nothing',
    'Non-Critical (Standard)',
    'Critical',
    'OnCall',
    'Urgent'
]

# Create a DataFrame for severity levels
df_severity = pd.DataFrame(severity_levels, columns=['Original Severity'])

# Initialize label encoder for Severity
le_severity = LabelEncoder()

# Encode the Severity column
df_severity['Encoded Severity'] = le_severity.fit_transform(df_severity['Original Severity'])

# Display the summary DataFrame with original and encoded values for Severity Levels
print("Severity Encoding Summary:")
print(df_severity)

# Display unique mappings for Severity Levels
print("\nSeverity Mapping:")
for original, encoded in zip(le_severity.classes_, range(len(le_severity.classes_))):
    print(f"{original}: {encoded}")

import pandas as pd

# Assuming subset_df is already defined and contains the necessary columns including 'Severity'
# Display the first few rows of the subset DataFrame
print("Subset DataFrame:")
print(subset_df.head())

# Define a mapping for severity to numeric priority (you may adjust this based on your needs)
priority_mapping = {

    'Critical': 1,
    'Urgent': 2,
    'OnCall': 3,
    'Non-Critical (Standard)': 4,
    'Nothing': 5
}


# Create a new column for numeric priority based on severity
subset_df['Priority'] = subset_df['Severity'].map(priority_mapping)

# Sort the DataFrame by priority (lower number means higher priority) and then by 'Required On'
top_priority_tickets = subset_df.sort_values(by=['Priority', 'Required On']).head(10)

# Display the top 10 priority tickets
print("\nTop 10 Priority Tickets for All Users:")
print(top_priority_tickets)

# Save the top priority tickets to a CSV file
top_priority_tickets.to_csv('top_priority_tickets_all_users.csv', index=False)

print("Top priority tickets have been saved to 'top_priority_tickets_all_users.csv'.")

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Provided list of ticket types
ticket_types = [
    'Administration',
    'IT Services - Desktop',
    'IT Services - Hardware',
    'IT Services - Laptop',
    'IT Services - Network',
    'IT Services - Telecom',
    'IT Services - DHR Hardware Maintenance',
    'IT Services - Cloud',
    'IT Services - Managed Services',
    'IT Services - Software Services',
    'HR',
    'Training Services',
    'Web Services',
    'IT Services - Virtual Machine (VM)',
    'Application Services - Programming'
]

# Create a DataFrame from the list
df_ticket_types = pd.DataFrame(ticket_types, columns=['Original Ticket Type'])

# Initialize label encoder
le_ticket_type = LabelEncoder()

# Encode the Ticket Type column
df_ticket_types['Encoded Ticket Type'] = le_ticket_type.fit_transform(df_ticket_types['Original Ticket Type'])

# Display the summary DataFrame with original and encoded values
print("Ticket Type Encoding Summary:")
print(df_ticket_types)

# Display unique mappings for Ticket Types
print("\nTicket Type Mapping:")
for original, encoded in zip(le_ticket_type.classes_, range(len(le_ticket_type.classes_))):
    print(f"{original}: {encoded}")

"""1. Administration: 0
2. IT Services - Desktop: 5
3. IT Services - Hardware: 6
4. IT Services - Laptop: 7
5. IT Services - Network: 9
6. IT Services - Telecom: 11
7. IT Services - DHR Hardware Maintenance: 4
8. IT Services - Cloud: 3
9. IT Services - Managed Services: 8
10. IT Services - Software Services: 10
11. HR: 2
12. Training Services: 13
13. Web Services: 14
14. IT Services - Virtual Machine (VM): 12
15. Application Services - Programming: 1

1. Critical: 0
2. Non-Critical (Standard): 1
3. Nothing: 2
4. OnCall: 3
5. Urgent: 4
"""



"""Random forest"""

from sklearn.preprocessing import LabelEncoder

# Initialize label encoders
le_severity = LabelEncoder()
le_ticket_type = LabelEncoder()
le_user_id = LabelEncoder()

# Encode categorical variables
subset_df['Severity'] = le_severity.fit_transform(subset_df['Severity'])
subset_df['Ticket Type'] = le_ticket_type.fit_transform(subset_df['Ticket Type'])
subset_df['Note Created By UserID'] = le_user_id.fit_transform(subset_df['Note Created By UserID'])

# Display the encoded DataFrame
print("\nEncoded Subset DataFrame:")
print(subset_df)



from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Define features and target variable
X = subset_df[['Severity', 'Ticket Type', 'Ticket Labor Hours']]  # You can include more features if needed
y = subset_df['Note Created By UserID']  # Target variable for assignment

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train a Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Display feature importances
importances = rf_model.feature_importances_
print("\nFeature Importances:")
for feature, importance in zip(X.columns, importances):
    print(f"{feature}: {importance:.4f}")

# Predict assignments on the test set
y_pred = rf_model.predict(X_test)

# Decode predicted values back to original user IDs
predicted_assignments = le_user_id.inverse_transform(y_pred)

# Display predictions alongside actual values
results_df = pd.DataFrame({
    'Actual Assignment': le_user_id.inverse_transform(y_test),
    'Predicted Assignment': predicted_assignments
})

print("\nPredictions vs Actual Assignments:")
print(results_df)

# Save the trained model and label encoders
import joblib
joblib.dump(rf_model, 'random_forest_model.pkl')
joblib.dump(le_severity, 'le_severity.pkl')
joblib.dump(le_ticket_type, 'le_ticket_type.pkl')
joblib.dump(le_user_id, 'le_user_id.pkl')

"""1. Administration: 0
2. IT Services - Desktop: 5
3. IT Services - Hardware: 6
4. IT Services - Laptop: 7
5. IT Services - Network: 9
6. IT Services - Telecom: 11
7. IT Services - DHR Hardware Maintenance: 4
8. IT Services - Cloud: 3
9. IT Services - Managed Services: 8
10. IT Services - Software Services: 10
11. HR: 2
12. Training Services: 13
13. Web Services: 14
14. IT Services - Virtual Machine (VM): 12
15. Application Services - Programming: 1

1. Critical: 0
2. Non-Critical (Standard): 1
3. Nothing: 2
4. OnCall: 3
5. Urgent: 4
"""

import pandas as pd
import joblib  # For loading the model

# Load the trained model and label encoders
rf_model = joblib.load('random_forest_model.pkl')
le_severity = joblib.load('le_severity.pkl')
le_ticket_type = joblib.load('le_ticket_type.pkl')
le_user_id = joblib.load('le_user_id.pkl')

# Function to get user input for a new ticket
def get_user_input():
    print("Enter Severity:")
    print("1: Critical")
    print("2: Urgent")
    print("3: OnCall")
    print("4: Non-Critical (Standard)")
    print("5: Nothing")
    severity = int(input("Select a severity (1-5): "))

    ticket_type = int(input("Enter Ticket Type (0: Bug, 1: Feature): "))
    labor_hours = float(input("Enter Ticket Labor Hours: "))

    return severity, ticket_type, labor_hours

# Get user input for unseen data
user_severity, user_ticket_type, user_labor_hours = get_user_input()

# Prepare the new ticket for prediction
new_ticket = pd.DataFrame({
    'Severity': [user_severity],
    'Ticket Type': [user_ticket_type],
    'Ticket Labor Hours': [user_labor_hours]
})

# Predict the assignment for the new ticket
predicted_assignment_encoded = rf_model.predict(new_ticket)
predicted_assignment = le_user_id.inverse_transform(predicted_assignment_encoded)

# Display the prediction result
print(f"Predicted Assignment for New Ticket: {predicted_assignment[0]}")

# Optional: If you want to see predictions alongside actual values (if available)
unseen_results_df = pd.DataFrame({
    'Severity': [user_severity],
    'Ticket Type': [user_ticket_type],
    'Ticket Labor Hours': [user_labor_hours],
    'Predicted Assignment': predicted_assignment
})

print("\nUnseen Ticket Prediction:")
print(unseen_results_df)

#Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy: {accuracy:.2f}")